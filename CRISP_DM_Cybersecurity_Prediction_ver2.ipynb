{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¶²è·¯å®‰å…¨å¨è„…è²¡å‹™æå¤±é æ¸¬å°ˆæ¡ˆ\n",
    "\n",
    "é€™æ˜¯ä¸€å€‹åŸºæ–¼æ©Ÿå™¨å­¸ç¿’çš„å°ˆæ¡ˆï¼Œæ—¨åœ¨é æ¸¬ç¶²è·¯å®‰å…¨äº‹ä»¶å¯èƒ½é€ æˆçš„è²¡å‹™æå¤±ã€‚å°ˆæ¡ˆæ¡ç”¨ CRISP-DMï¼ˆCross-Industry Standard Process for Data Miningï¼‰æµç¨‹æ–¹æ³•è«–ï¼Œå¾å•†æ¥­ç†è§£åˆ°æ¨¡å‹éƒ¨ç½²ï¼Œæä¾›äº†ä¸€å€‹å®Œæ•´çš„è³‡æ–™ç§‘å­¸å°ˆæ¡ˆç¯„ä¾‹ã€‚\n",
    "\n",
    "ä½¿ç”¨è€…å¯ä»¥é€éä¸€å€‹äº’å‹•å¼çš„ Streamlit ç¶²é æ‡‰ç”¨ç¨‹å¼ï¼Œè¼¸å…¥å‡è¨­çš„æ”»æ“Šæƒ…å¢ƒï¼Œä¾†é æ¸¬æ½›åœ¨çš„è²¡å‹™æå¤±ï¼Œä¸¦æ·±å…¥æ¢ç´¢è³‡æ–™èˆ‡æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. å•†æ¥­ç†è§£ (Business Understanding)\n",
    "\n",
    "**ç›®æ¨™ï¼š**\n",
    "éš¨è‘—å…¨çƒæ•¸ä½åŒ–è½‰å‹ï¼Œç¶²è·¯å®‰å…¨äº‹ä»¶é »å‚³ï¼Œå°ä¼æ¥­é€ æˆçš„è²¡å‹™è¡æ“Šä¹Ÿæ—¥ç›Šåš´é‡ã€‚æœ¬å°ˆæ¡ˆçš„ä¸»è¦å•†æ¥­ç›®æ¨™æ˜¯å»ºç«‹ä¸€å€‹æ•¸æ“šé©…å‹•çš„é æ¸¬æ¨¡å‹ï¼Œä»¥å”åŠ©ä¼æ¥­æˆ–çµ„ç¹”è©•ä¼°ä¸åŒç¶²è·¯å®‰å…¨å¨è„…äº‹ä»¶å¯èƒ½å¸¶ä¾†çš„è²¡å‹™æå¤±ï¼ˆä»¥ç™¾è¬ç¾å…ƒè¨ˆï¼‰ã€‚\n",
    "\n",
    "é€éé€™å€‹æ¨¡å‹ï¼Œæ±ºç­–è€…å¯ä»¥ï¼š\n",
    "- æ›´ç²¾æº–åœ°è©•ä¼°è³‡å®‰é¢¨éšªã€‚\n",
    "- å„ªå…ˆè™•ç†å’Œåˆ†é…è³‡æºçµ¦å¯èƒ½é€ æˆé‡å¤§æå¤±çš„å¨è„…é¡å‹ã€‚\n",
    "- ç‚ºè³‡å®‰ä¿éšªã€é ç®—è¦åŠƒå’ŒæŠ•è³‡æ±ºç­–æä¾›é‡åŒ–ä¾æ“šã€‚"
   ]
  },
    {                                                                                                           
     "cell_type": "markdown",                                                                                   
     "metadata": {},                                                                                            
     "source": [
      "## 2. è³‡æ–™ç†è§£ (Data Understanding)\n",
      "\n",
      "**è³‡æ–™ä¾†æºï¼š**\n",
      "æœ¬å°ˆæ¡ˆä½¿ç”¨ `Global_Cybersecurity_Threats_2015-2024.csv` è³‡æ–™é›†ã€‚æ­¤è³‡æ–™é›†åŒ…å«äº†å¾ 2015 å¹´åˆ° 2024 å¹´é–“çš„å…¨çƒç¶²è·¯å®‰å…¨å¨è„…äº‹ä»¶è¨˜éŒ„ã€‚\n",
      "\n",
      "**è³‡æ–™ç‰¹å¾µï¼š**\n",
      "è³‡æ–™é›†åŒ…å«å¤šç¨®æ•¸å€¼å’Œé¡åˆ¥ç‰¹å¾µï¼Œä¾‹å¦‚ï¼š\n",
      "- **Attack Type**: æ”»æ“Šé¡å‹ (e.g., DDoS, Malware, Phishing)ã€‚\n",
      "- **Country**: æ”»æ“Šç™¼ç”Ÿçš„åœ‹å®¶ã€‚\n",
      "- **Sector**: å—æ”»æ“Šçš„ç”¢æ¥­åˆ¥ã€‚\n",
      "- **Number of Affected Users**: å—å½±éŸ¿çš„ä½¿ç”¨è€…æ•¸é‡ã€‚\n",
      "- **Incident Resolution Time (in Hours)**: äº‹ä»¶è§£æ±ºæ‰€éœ€æ™‚é–“ï¼ˆå°æ™‚ï¼‰ã€‚\n",
      "- **Financial Loss (in Million $)**: è²¡å‹™æå¤±ï¼ˆç™¾è¬ç¾å…ƒï¼‰ï¼Œæ­¤ç‚ºæˆ‘å€‘çš„**ç›®æ¨™è®Šæ•¸**ã€‚\n",
      "\n",
      "åœ¨ Streamlit æ‡‰ç”¨ç¨‹å¼çš„ã€Œåˆ†æé é¢ã€ä¸­ï¼Œã€Œè³‡æ–™æ¦‚è¦½ã€å’Œã€Œç‰¹å¾µåˆ†æã€åˆ†é æä¾›äº†å°è³‡æ–™çš„æ·±å…¥æ¢ç´¢ã€‚"
     ]
    },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Set Matplotlib font to avoid Chinese display issues (äº‚ç¢¼)\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS'] # Or any other font that supports Chinese characters\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# è¼‰å…¥è³‡æ–™é›†\n",
    "df = pd.read_csv('Global_Cybersecurity_Threats_2015-2024.csv')\n",
    "\n",
    "print(\"### è³‡æ–™é›†é è¦½ ###\")\n",
    "display(df.head())\n",
    "print(f\"\n### è³‡æ–™é›†ç¶­åº¦: {df.shape[0]} è¡Œ, {df.shape[1]} åˆ—\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### è³‡æ–™é›†æè¿° ###\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### è³‡æ–™é›†è³‡è¨Š ###\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### è²¡å‹™æå¤± (ç›®æ¨™è®Šæ•¸) åˆ†ä½ˆ ###\")\n",
    "financial_loss = df['Financial Loss (in Million $)']\n",
    "\n",
    "# Calculate Skewness\n",
    "skewness = financial_loss.skew()\n",
    "print(f\"**ååº¦ (Skewness):** {skewness:.2f}\")\n",
    "if skewness > 0.5:\n",
    "    print(\"åˆ†ä½ˆå‘ˆå³åï¼ˆæ­£åï¼‰ï¼Œè¡¨ç¤ºæœ‰å°‘æ•¸æ¥µå¤§çš„æå¤±å€¼ã€‚\")\n",
    "elif skewness < -0.5:\n",
    "    print(\"åˆ†ä½ˆå‘ˆå·¦åï¼ˆè² åï¼‰ã€‚\")\n",
    "else:\n",
    "    print(\"åˆ†ä½ˆå¤§è‡´å°ç¨±ã€‚\")\n",
    "\n",
    "# Plot distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.histplot(financial_loss, kde=False, ax=ax, stat=\"density\", label=\"Actual Distribution\")\n",
    "\n",
    "# Overlay normal distribution\n",
    "xmin, xmax = ax.get_xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, financial_loss.mean(), financial_loss.std())\n",
    "ax.plot(x, p, 'k', linewidth=2, label=\"Normal Distribution\")\n",
    "\n",
    "ax.set_title('Distribution of Financial Loss vs. Normal Distribution')\n",
    "ax.set_xlabel('Financial Loss (in Million $)')\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. è³‡æ–™æº–å‚™ (Data Preparation)\n",
    "\n",
    "æ­¤éšæ®µç”± `prepare_data.py` è…³æœ¬è² è²¬ï¼Œä¸»è¦åŸ·è¡Œä»¥ä¸‹æ­¥é©Ÿï¼š\n",
    "\n",
    "1.  **è¼‰å…¥è³‡æ–™**ï¼šå¾ CSV æª”æ¡ˆè¼‰å…¥è³‡æ–™é›†ã€‚\n",
    "2.  **ç‰¹å¾µå·¥ç¨‹**ï¼š\n",
    "    *   **ç¨ç†±ç·¨ç¢¼ (One-Hot Encoding)**ï¼šå°‡æ‰€æœ‰é¡åˆ¥ç‰¹å¾µï¼ˆå¦‚ `Attack Type`, `Country`ï¼‰è½‰æ›ç‚ºæ•¸å€¼æ ¼å¼ï¼Œä»¥ä¾¿æ©Ÿå™¨å­¸ç¿’æ¨¡å‹èƒ½å¤ è™•ç†ã€‚\n",
    "3.  **è³‡æ–™æ¨™æº–åŒ–**ï¼š\n",
    "    *   ä½¿ç”¨ `StandardScaler` å°æ‰€æœ‰æ•¸å€¼ç‰¹å¾µé€²è¡Œæ¨™æº–åŒ–ï¼Œä½¿å…¶å…·æœ‰é›¶å‡å€¼å’Œå–®ä½è®Šç•°æ•¸ã€‚é€™ä¸€æ­¥é©Ÿå°æ–¼ç·šæ€§æ¨¡å‹å’Œ RFE çš„ç©©å®šæ€§è‡³é—œé‡è¦ã€‚\n",
    "4.  **è³‡æ–™åˆ†å‰²**ï¼š\n",
    "    *   å°‡è™•ç†å¾Œçš„è³‡æ–™é›†ä»¥ 80/20 çš„æ¯”ä¾‹åˆ†å‰²ç‚ºè¨“ç·´é›†å’Œæ¸¬è©¦é›†ã€‚æ­¤éç¨‹ä¸­ä½¿ç”¨å›ºå®šçš„ `random_state` ä»¥ç¢ºä¿çµæœçš„å¯é‡ç¾æ€§ã€‚\n",
    "5.  **å„²å­˜ç”¢ç‰©**ï¼šå°‡è™•ç†å¥½çš„è¨“ç·´é›†/æ¸¬è©¦é›†ï¼ˆ`.npy` æ ¼å¼ï¼‰ã€`StandardScaler` ç‰©ä»¶ã€ä»¥åŠç‰¹å¾µåç¨±åˆ—è¡¨å„²å­˜ç‚ºæª”æ¡ˆï¼Œä¾›å¾ŒçºŒæ¨¡å‹è¨“ç·´å’Œæ‡‰ç”¨ç¨‹å¼ä½¿ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "df = pd.read_csv('Global_Cybersecurity_Threats_2015-2024.csv')\n",
    "\n",
    "# è¨­å®š target èˆ‡ features\n",
    "target = \"Financial Loss (in Million $)\"\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "# é¡åˆ¥æ¬„ä½è½‰æ›\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# æ•¸å€¼æ¨™æº–åŒ–\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# è¨“ç·´/æ¸¬è©¦é›†åˆ‡åˆ†\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Save the data and scaler\n",
    "np.save('X_train.npy', X_train)\n",
    "np.save('X_test.npy', X_test)\n",
    "np.save('y_train.npy', y_train)\n",
    "np.save('y_test.npy', y_test)\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(X.columns, 'feature_names.pkl')\n",
    "\n",
    "# Save full processed X and y for statsmodels\n",
    "np.save('X_full_processed.npy', X)\n",
    "np.save('y_full_processed.npy', y)\n",
    "print('\nâœ… è³‡æ–™æº–å‚™å®Œæˆï¼Œç›¸é—œæª”æ¡ˆå·²å„²å­˜ã€‚')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ¨¡å‹å»ºç«‹ (Modeling)\n",
    "\n",
    "æ­¤éšæ®µç”± `train_model.py` è…³æœ¬è² è²¬ã€‚æˆ‘å€‘å»ºç«‹äº†å…©å€‹äº’è£œçš„è¿´æ­¸æ¨¡å‹ï¼š\n",
    "\n",
    "1.  **Scikit-learn ç·šæ€§è¿´æ­¸ + RFE**:\n",
    "    *   **éæ­¸ç‰¹å¾µæ¶ˆé™¤ (RFE)**ï¼šé¦–å…ˆï¼Œæˆ‘å€‘ä½¿ç”¨ RFE ä¾†è‡ªå‹•ç¯©é¸å‡ºå°é æ¸¬è²¡å‹™æå¤±æœ€é‡è¦çš„ 10 å€‹ç‰¹å¾µã€‚\n",
    "    *   **ç·šæ€§è¿´æ­¸ (Linear Regression)**ï¼šæ¥è‘—ï¼Œæˆ‘å€‘ä½¿ç”¨ä¸€å€‹æ¨™æº–çš„ç·šæ€§è¿´æ­¸æ¨¡å‹ï¼Œåƒ…åœ¨ RFE ç¯©é¸å‡ºçš„ç‰¹å¾µä¸Šé€²è¡Œè¨“ç·´ã€‚é€™å€‹æ¨¡å‹ (`cyber_risk_model.pkl`) ä¸»è¦ç”¨æ–¼ç”¢ç”Ÿæœ€çµ‚çš„é æ¸¬å€¼ã€‚\n",
    "\n",
    "2.  **Statsmodels OLS æ¨¡å‹**:\n",
    "    *   æˆ‘å€‘å¦å¤–ä½¿ç”¨ `statsmodels` å‡½å¼åº«å»ºç«‹äº†ä¸€å€‹æ™®é€šæœ€å°äºŒä¹˜æ³• (OLS) æ¨¡å‹ã€‚æ­¤æ¨¡å‹ (`statsmodels_model.pkl`) çš„å„ªå‹¢åœ¨æ–¼æä¾›è©³ç´°çš„çµ±è¨ˆæ‘˜è¦ï¼ŒåŒ…æ‹¬ç‰¹å¾µçš„ p-valueã€ä¿¡è³´å€é–“ç­‰ã€‚\n",
    "    *   åœ¨æœ¬å°ˆæ¡ˆä¸­ï¼Œå®ƒä¸»è¦ç”¨æ–¼è¨ˆç®—é æ¸¬å€¼çš„ **95% é æ¸¬å€é–“**ï¼Œä¸¦åœ¨ã€Œç‰¹å¾µé‡è¦æ€§ã€åˆ†æä¸­æä¾›ä¿‚æ•¸åƒè€ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "import joblib\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data for sklearn model\n",
    "X_train = np.load('X_train.npy', allow_pickle=True)\n",
    "y_train = np.load('y_train.npy', allow_pickle=True)\n",
    "\n",
    "# Create a Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Use Recursive Feature Elimination (RFE) to select the best features\n",
    "rfe = RFE(model, n_features_to_select=10)\n",
    "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
    "\n",
    "# Train the sklearn model on the selected features\n",
    "model.fit(X_train_rfe, y_train)\n",
    "\n",
    "# Save the trained sklearn model and the RFE selector\n",
    "joblib.dump(model, 'cyber_risk_model.pkl')\n",
    "joblib.dump(rfe, 'rfe.joblib')\n",
    "\n",
    "print(\"Sklearn model trained and saved successfully.\")\n",
    "\n",
    "# --- Fit and Save Statsmodels OLS Model ---\n",
    "\n",
    "# Load full processed X and y for statsmodels\n",
    "X_full_processed = np.load('X_full_processed.npy', allow_pickle=True)\n",
    "y_full_processed = np.load('y_full_processed.npy', allow_pickle=True)\n",
    "feature_names = joblib.load('feature_names.pkl')\n",
    "\n",
    "# Create DataFrame for statsmodels and ensure numeric types\n",
    "X_sm = pd.DataFrame(X_full_processed, columns=feature_names).astype(float)\n",
    "y_sm = pd.Series(y_full_processed).astype(float)\n",
    "\n",
    "# Add a constant to the X for statsmodels (for intercept)\n",
    "X_sm = sm.add_constant(X_sm)\n",
    "\n",
    "# Get the names of the features selected by RFE\n",
    "selected_feature_names_rfe = feature_names[rfe.support_].tolist()\n",
    "if 'const' not in selected_feature_names_rfe:\n",
    "    selected_feature_names_rfe.insert(0, 'const')\n",
    "\n",
    "# Filter X_sm to include only the RFE-selected features\n",
    "X_sm_selected = X_sm[selected_feature_names_rfe]\n",
    "\n",
    "# Fit statsmodels OLS model\n",
    "sm_model = sm.OLS(y_sm, X_sm_selected).fit()\n",
    "\n",
    "# Save the statsmodels model\n",
    "joblib.dump(sm_model, 'statsmodels_model.pkl')\n",
    "\n",
    "print(\"Statsmodels OLS model trained and saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. æ¨¡å‹è©•ä¼° (Evaluation)\n",
    "\n",
    "æ¨¡å‹çš„è©•ä¼°åœ¨ Streamlit æ‡‰ç”¨ç¨‹å¼çš„ã€Œåˆ†æé é¢ã€ä¸­é€²è¡Œï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹å¹¾å€‹éƒ¨åˆ†ï¼š\n",
    "\n",
    "- **è¿´æ­¸æŒ‡æ¨™**ï¼šåœ¨ã€Œæ¨¡å‹æ€§èƒ½ã€å€å¡Šï¼Œæˆ‘å€‘è¨ˆç®—ä¸¦å±•ç¤ºäº†ä¸‰å€‹é—œéµçš„è¿´æ­¸è©•ä¼°æŒ‡æ¨™ï¼š\n",
    "  - **R-squared (RÂ²)**: è§£é‡‹äº†æ¨¡å‹å°ç›®æ¨™è®Šæ•¸è®Šç•°æ€§çš„è§£é‡‹ç¨‹åº¦ã€‚\n",
    "  - **Root Mean Squared Error (RMSE)**: è¡¡é‡é æ¸¬å€¼èˆ‡å¯¦éš›å€¼ä¹‹é–“çš„å¹³å‡èª¤å·®å¹…åº¦ã€‚\n",
    "  - **Mean Absolute Error (MAE)**: æä¾›äº†å¦ä¸€ç¨®èª¤å·®çš„è¡¡é‡æ–¹å¼ï¼Œè¼ƒä¸å—ç•°å¸¸å€¼å½±éŸ¿ã€‚\n",
    "\n",
    "- **è¦–è¦ºåŒ–è©•ä¼°**ï¼š\n",
    "  - **å¯¦éš› vs. é æ¸¬åœ–**ï¼šä¸€å€‹æ•£é»åœ–ï¼Œç”¨æ–¼æ¯”è¼ƒå¯¦éš›æå¤±èˆ‡æ¨¡å‹é æ¸¬æå¤±çš„ä¸€è‡´æ€§ã€‚\n",
    "  - **æ®˜å·®åœ–**ï¼šç”¨æ–¼æª¢æŸ¥èª¤å·®æ˜¯å¦éš¨æ©Ÿåˆ†ä½ˆï¼Œæ˜¯è©•ä¼°æ¨¡å‹å‡è¨­çš„é‡è¦å·¥å…·ã€‚\n",
    "  - **æ··æ·†çŸ©é™£**ï¼šé›–ç„¶é€™æ˜¯è¿´æ­¸å•é¡Œï¼Œä½†æˆ‘å€‘å°‡é€£çºŒçš„æå¤±å€¼åˆ†ç‚ºã€Œé«˜ã€ä¸­ã€ä½ã€ä¸‰å€‹ç­‰ç´šï¼Œä¸¦å»ºç«‹äº†ä¸€å€‹äº’å‹•å¼çš„æ··æ·†çŸ©é™£ã€‚é€™è®“ä½¿ç”¨è€…å¯ä»¥å¾ã€Œåˆ†é¡ã€çš„è§’åº¦è©•ä¼°æ¨¡å‹åœ¨ä¸åŒæå¤±ç­‰ç´šä¸Šçš„é æ¸¬æº–ç¢ºåº¦ï¼Œä¸¦å¯ä¾ç‰¹å®šç‰¹å¾µé€²è¡Œç¯©é¸åˆ†æã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set Matplotlib font to avoid Chinese display issues (äº‚ç¢¼)\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS'] # Or any other font that supports Chinese characters\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# Load test data\n",
    "X_test = np.load('X_test.npy', allow_pickle=True)\n",
    "y_test = np.load('y_test.npy', allow_pickle=True)\n",
    "\n",
    "# Load model and rfe\n",
    "model = joblib.load('cyber_risk_model.pkl')\n",
    "rfe = joblib.load('rfe.joblib')\n",
    "sm_model = joblib.load('statsmodels_model.pkl')\n",
    "full_feature_names = joblib.load('feature_names.pkl')\n",
    "\n",
    "# Transform test data\n",
    "selected_X_test = rfe.transform(X_test)\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = model.predict(selected_X_test)\n",
    "\n",
    "st.subheader(\"### æ¨¡å‹è©•ä¼°æŒ‡æ¨™ ###\")\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"- **R-squared (RÂ²):** {r2:.3f}\")\n",
    "print(f\"- **Root Mean Squared Error (RMSE):** {rmse:.3f}\")\n",
    "print(f\"- **Mean Absolute Error (MAE):** {mae:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"#### å¯¦éš› vs. é æ¸¬æå¤±æ•£é»åœ– ###\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=y_test, y=y_pred)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel(\"Actual Financial Loss (Million $)\")\n",
    "plt.ylabel(\"Predicted Financial Loss (Million $)\")\n",
    "plt.title(\"Actual vs. Predicted Financial Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"#### æ®˜å·®åœ– ###\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=y_pred, y=(y_test - y_pred))\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel(\"Predicted Financial Loss (Million $)\")\n",
    "plt.ylabel(\"Residuals (Actual - Predicted)\")\n",
    "plt.title(\"Residuals Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” RFE ç‰¹å¾µåˆ†æ (éæ­¸ç‰¹å¾µæ¶ˆé™¤)\n",
    "RFE é€ééæ­¸åœ°è€ƒæ…®è¶Šä¾†è¶Šå°çš„ç‰¹å¾µé›†ä¾†é¸æ“‡ç‰¹å¾µã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RFE æ¨¡å‹é¸æ“‡äº† {rfe.n_features_} å€‹ç‰¹å¾µã€‚\")\n",
    "selected_rfe_features = full_feature_names[rfe.support_]\n",
    "print(\"#### RFE é¸æ“‡çš„ç‰¹å¾µ:\")\n",
    "for feature in selected_rfe_features.tolist():\n",
    "    print(f\"- `{feature}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸŒŸ ç‰¹å¾µé‡è¦æ€§\n",
    "å°æ–¼ç·šæ€§æ¨¡å‹ï¼Œç‰¹å¾µé‡è¦æ€§å¯ä»¥å¾ä¿‚æ•¸çš„çµ•å°å¤§å°æ¨æ–·å‡ºä¾†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model_coefs = sm_model.params.drop('const', errors='ignore')\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': sm_model_coefs.index,\n",
    "    'Coefficient': sm_model_coefs.values\n",
    "})\n",
    "feature_importance_df['Absolute_Coefficient'] = feature_importance_df['Coefficient'].abs()\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Absolute_Coefficient', ascending=False)\n",
    "\n",
    "print(\"#### ç‰¹å¾µä¿‚æ•¸ (ä¾†è‡ª Statsmodels OLS æ¨¡å‹):\")\n",
    "display(feature_importance_df[['Feature', 'Coefficient']].head(15))\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.barplot(x='Absolute_Coefficient', y='Feature', data=feature_importance_df.head(15), palette='viridis')\n",
    "plt.title('Top 15 Feature Importances (Absolute Coefficients)')\n",
    "plt.xlabel('Absolute Coefficient Value')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š ç•°å¸¸å€¼åˆ†æ\n",
    "è­˜åˆ¥ä¸¦è¦–è¦ºåŒ–æ•¸å€¼ç‰¹å¾µå’Œè²¡å‹™æå¤±ä¸­çš„æ½›åœ¨ç•°å¸¸å€¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original data to get original numerical columns\n",
    "df_analysis = pd.read_csv('Global_Cybersecurity_Threats_2015-2024.csv')\n",
    "original_numerical_cols = ['Year', 'Number of Affected Users', 'Incident Resolution Time (in Hours)']\n",
    "\n",
    "print(\"#### æ•¸å€¼ç‰¹å¾µçš„ç›’é¬šåœ– (ç•°å¸¸å€¼æª¢æ¸¬) ###\")\n",
    "for col in original_numerical_cols + ['Financial Loss (in Million $)']:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.boxplot(x=df_analysis[col])\n",
    "    plt.title(f'Box Plot of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.show()\n",
    "\n",
    "print(\"#### è²¡å‹™æå¤±ç•°å¸¸å€¼æª¢æ¸¬ (ä½¿ç”¨ IQR) ###\")\n",
    "Q1 = df_analysis['Financial Loss (in Million $)'].quantile(0.25)\n",
    "Q3 = df_analysis['Financial Loss (in Million $)'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outlier_threshold_upper = Q3 + 1.5 * IQR\n",
    "outlier_threshold_lower = Q1 - 1.5 * IQR\n",
    "\n",
    "df_outliers = df_analysis[(df_analysis['Financial Loss (in Million $)'] > outlier_threshold_upper) |\n",
    "                          (df_analysis['Financial Loss (in Million $)'] < outlier_threshold_lower)]\n",
    "\n",
    "if not df_outliers.empty:\n",
    "    print(f\"ä½¿ç”¨ IQR æ–¹æ³•åœ¨è²¡å‹™æå¤±ä¸­ç™¼ç¾ {len(df_outliers)} å€‹æ½›åœ¨ç•°å¸¸å€¼ã€‚\")\n",
    "    display(df_outliers[['Year', 'Financial Loss (in Million $)', 'Attack Type', 'Country']])\n",
    "else:\n",
    "    print(\"ä½¿ç”¨ IQR æ–¹æ³•åœ¨è²¡å‹™æå¤±ä¸­æœªç™¼ç¾é¡¯è‘—ç•°å¸¸å€¼ã€‚\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=df_analysis['Year'], y=df_analysis['Financial Loss (in Million $)'], label='All Data')\n",
    "if not df_outliers.empty:\n",
    "    sns.scatterplot(x=df_outliers['Year'], y=df_outliers['Financial Loss (in Million $)'], color='red', label='Outlier')\n",
    "plt.axhline(y=outlier_threshold_upper, color='orange', linestyle=':', label='Upper IQR Bound')\n",
    "plt.axhline(y=outlier_threshold_lower, color='orange', linestyle=':', label='Lower IQR Bound')\n",
    "plt.title('Financial Loss vs. Year with Outlier Bounds')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Financial Loss (Million $)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ˆ æ··æ·†çŸ©é™£\n",
    "ç‚ºäº†ç”¢ç”Ÿæ··æ·†çŸ©é™£ï¼Œæˆ‘å€‘å°‡é€£çºŒçš„è²¡å‹™æå¤±ç›®æ¨™è®Šæ•¸è½‰æ›ç‚ºä¸‰å€‹é¡åˆ¥ï¼šä½ã€ä¸­ã€é«˜ã€‚æ‚¨å¯ä»¥ä¾ç‰¹å®šç‰¹å¾µç¯©é¸è³‡æ–™ï¼Œè§€å¯Ÿæ¨¡å‹åœ¨ä¸åŒæƒ…å¢ƒä¸‹çš„è¡¨ç¾ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original data to get original categorical columns\n",
    "df_analysis = pd.read_csv('Global_Cybersecurity_Threats_2015-2024.csv')\n",
    "original_categorical_columns_map = {}\n",
    "for feature in joblib.load('feature_names.pkl'):\n",
    "    if feature not in ['Year', 'Number of Affected Users', 'Incident Resolution Time (in Hours)', 'Financial Loss (in Million $)']:\n",
    "        parts = feature.split('_', 1)\n",
    "        if len(parts) > 1:\n",
    "            category = parts[0]\n",
    "            option = parts[1]\n",
    "            if category not in original_categorical_columns_map:\n",
    "                original_categorical_columns_map[category] = []\n",
    "            original_categorical_columns_map[category].append(option)\n",
    "\n",
    "# Recreate the train-test split on the original data to get access to original features for filtering\n",
    "target = \"Financial Loss (in Million $)\"\n",
    "features = df_analysis.drop(columns=[target])\n",
    "y_full = df_analysis[target]\n",
    "_, X_test_original_df, _, y_test = train_test_split(features, y_full, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load the processed test set to make predictions\n",
    "X_test_processed = np.load('X_test.npy', allow_pickle=True)\n",
    "selected_X_test = rfe.transform(X_test_processed)\n",
    "y_pred = model.predict(selected_X_test)\n",
    "\n",
    "# --- Confusion Matrix Calculation and Display ---\n",
    "# Define bins and labels for categorization based on the filtered data\n",
    "try:\n",
    "    bins = pd.qcut(y_test, q=3, retbins=True, duplicates='drop')[1]\n",
    "    labels = [\"ä½\", \"ä¸­\", \"é«˜\"]\n",
    "except ValueError: # Happens if not enough unique values for 3 quantiles\n",
    "    try:\n",
    "        bins = pd.qcut(y_test, q=2, retbins=True, duplicates='drop')[1]\n",
    "        labels = [\"ä½\", \"é«˜\"]\n",
    "    except ValueError: # Happens if all values are the same\n",
    "        bins = [y_test.min(), y_test.max()]\n",
    "        labels = [\"å–®ä¸€å€¼\"]\n",
    "\n",
    "y_test_cat = pd.cut(y_test, bins=bins, labels=labels, include_lowest=True)\n",
    "y_pred_cat = pd.cut(y_pred, bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# Handle cases where predictions might fall out of y_test bins\n",
    "if y_pred_cat.isnull().any():\n",
    "    y_pred_cat = y_pred_cat.cat.add_categories(['é æ¸¬è¶…å‡ºç¯„åœ'])\n",
    "    y_pred_cat = y_pred_cat.fillna('é æ¸¬è¶…å‡ºç¯„åœ')\n",
    "    all_labels = list(labels) + ['é æ¸¬è¶…å‡ºç¯„åœ']\n",
    "else:\n",
    "    all_labels = labels\n",
    "\n",
    "print(\"#### æå¤±é¡åˆ¥å®šç¾©: ###\")\n",
    "if len(bins) > 1 and \"å–®ä¸€å€¼\" not in labels:\n",
    "    for i in range(len(bins) - 1):\n",
    "        print(f\"- **{labels[i]}**: ${bins[i]:.2f}M - ${bins[i+1]:.2f}M\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test_cat, y_pred_cat, labels=all_labels)\n",
    "cm_df = pd.DataFrame(cm, index=all_labels, columns=all_labels)\n",
    "\n",
    "print(\"#### æ··æ·†çŸ©é™£: ###\")\n",
    "print(\"æ­¤çŸ©é™£é¡¯ç¤ºäº†æ¨¡å‹åœ¨é æ¸¬ä¸åŒæå¤±ç­‰ç´šæ™‚çš„è¡¨ç¾ã€‚\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix for Financial Loss Categories')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. éƒ¨ç½² (Deployment)\n",
    "\n",
    "æœ¬å°ˆæ¡ˆçš„æœ€çµ‚ç”¢å‡ºæ˜¯ä¸€å€‹éƒ¨ç½²åœ¨ Streamlit ä¸Šçš„äº’å‹•å¼ç¶²é æ‡‰ç”¨ç¨‹å¼ (`5114050013_hw2.py`)ã€‚\n",
    "\n",
    "**æ‡‰ç”¨ç¨‹å¼åŠŸèƒ½ï¼š**\n",
    "\n",
    "- **é æ¸¬é é¢**ï¼šä½¿ç”¨è€…å¯ä»¥åœ¨å´é‚Šæ¬„è¼¸å…¥å„ç¨®æ”»æ“Šäº‹ä»¶çš„åƒæ•¸ï¼ˆå¦‚å¹´ä»½ã€æ”»æ“Šé¡å‹ã€å½±éŸ¿ç”¨æˆ¶æ•¸ç­‰ï¼‰ï¼Œé»æ“ŠæŒ‰éˆ•å¾Œï¼Œæ‡‰ç”¨ç¨‹å¼æœƒç«‹å³å›å‚³é æ¸¬çš„è²¡å‹™æå¤±é‡‘é¡ï¼Œä¸¦ä»¥åœ–è¡¨å½¢å¼å±•ç¤ºå…¶ 95% çš„é æ¸¬å€é–“ã€‚\n",
    "\n",
    "- **åˆ†æé é¢**ï¼šæä¾›äº†ä¸€å€‹åŠŸèƒ½è±å¯Œçš„å„€è¡¨æ¿ï¼Œè®“ä½¿ç”¨è€…å¯ä»¥ï¼š\n",
    "  - æ¦‚è¦½è³‡æ–™é›†çš„çµ±è¨ˆç‰¹æ€§èˆ‡åˆ†ä½ˆã€‚\n",
    "  - æ¢ç´¢ä¸åŒç‰¹å¾µä¹‹é–“çš„é—œä¿‚ä»¥åŠå®ƒå€‘å°è²¡å‹™æå¤±çš„å½±éŸ¿ã€‚\n",
    "  - æŸ¥çœ‹æ¨¡å‹çš„è©³ç´°æ€§èƒ½æŒ‡æ¨™èˆ‡è©•ä¼°åœ–è¡¨ã€‚\n",
    "  - åˆ†æ RFE æ‰€é¸å‡ºçš„é‡è¦ç‰¹å¾µã€‚\n",
    "  - é€éäº’å‹•å¼æ··æ·†çŸ©é™£ï¼Œæ·±å…¥äº†è§£æ¨¡å‹åœ¨ç‰¹å®šæƒ…å¢ƒä¸‹çš„åˆ†é¡è¡¨ç¾ã€‚"
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¦‚ä½•éƒ¨ç½²åˆ° Streamlit Cloud\n",
    "1. Push å°ˆæ¡ˆè³‡æ–™å¤¾ to GitHub\n",
    "2. è‡³ https://share.streamlit.io ï¼Œé»æ“Š â€œCreate appâ€\n",
    "3. Repositoryï¼šä¸‹æ‹‰é¸æ“‡ candice-wu/Cybersecurity_HW_02_Multiple_Linear_Regression\n",
    "4. Branchï¼šMain\n",
    "5. Main file pathï¼š5114050013_hw2.py\n",
    "6. App URL (optional)ï¼šé è¨­å¯ä»¥ç¶­æŒï¼Œæˆ–æ”¹æ‰ä¸¦å¦å¤–å‘½åâ€¨ï¼Œå¦‚ï¼šhttps://hw02-multiple-linear-regression\n",
    "7. é»æ“Š â€œDeployâ€ å³å®Œæˆéƒ¨ç½²\n",
    "\n",
    "## å¦‚ä½•åŸ·è¡Œå°ˆæ¡ˆ\n",
    "\n",
    "1.  **å®‰è£ä¾è³´å‡½å¼åº«**:\n",
    "    ```bash\n",
    "    pip install -r requirements.txt\n",
    "    ```\n",
    "\n",
    "2.  **æº–å‚™è³‡æ–™èˆ‡è¨“ç·´æ¨¡å‹**:\n",
    "    åŸ·è¡Œä»¥ä¸‹è…³æœ¬ä¾†æº–å‚™è³‡æ–™ä¸¦è¨“ç·´æ¨¡å‹ã€‚\n",
    "    ```bash\n",
    "    python prepare_data.py\n",
    "    python train_model.py\n",
    "    ```\n",
    "\n",
    "3.  **å•Ÿå‹•æ‡‰ç”¨ç¨‹å¼**:\n",
    "    ```bash\n",
    "    streamlit run 5114050013_hw2.py\n",
    "    ```\n",
    "    æ¥è‘—åœ¨ç€è¦½å™¨ä¸­é–‹å•Ÿé¡¯ç¤ºçš„ URLã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
